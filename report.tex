\documentclass[12pt]{article}

% ---------- Page layout ----------
\usepackage[margin=1in]{geometry}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% ---------- Packages ----------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage{hyperref}

\definecolor{linkblue}{RGB}{0,0,140}
\hypersetup{
  colorlinks = true,
  linkcolor  = linkblue,
  citecolor  = linkblue,
  urlcolor   = linkblue
}

\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\MC}{\mathrm{MC}}
\newcommand{\QMC}{\mathrm{QMC}}
\newcommand{\highlight}[1]{\boxed{#1}}
% Path to figures generated by Python
\newcommand{\figpath}{/Users/tannerwagner/MATH_505_Final_Project/Code/}


% ---------- Title ----------
\title{\textbf{Quasi-Monte Carlo vs Monte Carlo\\[0.25em]
for High-Dimensional Integration on $[0,1]^d$}}
\author{Tanner Wagner \\[0.25em]
MATH 505: Numerical Analysis \\[0.25em]
University of New Mexico}
\date{\today}

% =====================================================
\begin{document}

\maketitle

\begin{abstract}
\noindent This project compares plain Monte Carlo (MC) and quasi-Monte Carlo (QMC)
methods based on Sobol sequences for high-dimensional integrals over $[0,1]^d$.
This project looks at several smooth test functions with known exact integrals and compares
the methods in terms of absolute error, empirical convergence rate, and
practical time-to-accuracy for dimensions $d \in \{5,10,15,20\}$.
\end{abstract}

\thispagestyle{empty}

\newpage

\tableofcontents
\newpage

\section{Introduction}
\label{sec:intro}
High-dimensional integrals appear in many applications, including
expectations in probability and statistics, option pricing in finance, and
posterior averages in Bayesian inference. In these settings one often
needs to approximate integrals over a hypercube such as $[0,1]^d$.
Classical deterministic quadrature methods (e.g., Gaussian or composite
Newton--Cotes rules) are powerful in low dimensions, but their cost grows
very rapidly with $d$ if we use tensor-product constructions. This
\emph{curse of dimensionality} quickly makes standard quadrature
prohibitively expensive.
\\
\\
Monte Carlo (MC) methods offer a dimension-robust alternative. By viewing
an integral as the expectation of a function of a random variable, Monte
Carlo approximates the integral by the average of $N$ random samples.
Under mild conditions, the error decays like $O(N^{-1/2})$, essentially
independent of $d$ in the exponent, but this convergence is relatively
slow. Quasi--Monte Carlo (QMC) methods seek to improve on this by
replacing random samples with deterministic low-discrepancy points (such
as Sobol sequences) that are more uniformly distributed over $[0,1]^d$.
For smooth integrands with suitable structure, QMC can achieve much smaller
errors than MC for the same $N$, and heuristically exhibits an
$O(N^{-1})$-like behavior up to logarithmic factors.
\\
\\
The goal of this project is to compare plain Monte Carlo and Sobol
quasi-Monte Carlo for high-dimensional integration on $[0,1]^d$. We focus
on three smooth, separable test integrands with known exact values,
including a Gaussian-type integrand related to the normal distribution.
For dimensions $d \in \{5,10,15,20\}$ and a range of sample sizes $N$, we
measure absolute error, empirical convergence rates, and practical
time-to-accuracy for both methods.
\\
\\
The rest of the paper is organized as follows.
Section~\ref{sec:methods} reviews the probabilistic background and
formulates MC and QMC as quadrature rules.
Section~\ref{sec:setup} describes the test integrands and experimental
setup.
Section~\ref{sec:results} presents numerical comparisons of MC and QMC,
and Section~\ref{sec:conclusion} summarizes the main findings and possible
extensions.


% =====================================================
\section{Mathematical Background and Methods}
\label{sec:methods}

\subsection{High-dimensional integration on $[0,1]^d$}
We are interested in approximating integrals of the form
\[
  I(f,d) = \int_{[0,1]^d} f(x)\,dx,
  \qquad x = (x_1,\dots,x_d) \in [0,1]^d.
\]
\\
\noindent Here $[0,1]^d$ denotes the $d$-dimensional unit cube, i.e., each coordinate
$x_j$ ranges between $0$ and $1$. When $d=1$ this is a single integral over
$[0,1]$; when $d=2$ it is a double integral over the unit square; and in
general it is a $d$-fold integral over the unit cube.
\\
\\
It is often convenient to interpret $I(f,d)$ as an \emph{average value} of
$f$ over $[0,1]^d$. This viewpoint naturally leads us to a short review of
basic probability concepts.

\subsection{Brief probabilistic background}
I'll briefly introduce the notions of random variables, expectation, and the
law of large numbers that underlie Monte Carlo integration.

\paragraph{Random variables and expectation.}
A (real-valued) \emph{random variable} $Y$ is a quantity whose value is not
fixed, but instead is determined by some random experiment. For example,
$Y$ could represent the outcome of rolling a die, or the value of a
function evaluated at a randomly chosen point.
\\
\\
The \emph{expectation} (or expected value) of $Y$, denoted $\mathbb{E}[Y]$,
is the average value we would obtain if we repeated the experiment many
times and averaged the outcomes. In the continuous setting, if $Y$ has a
probability density function $p(y)$, then
\[
  \mathbb{E}[Y] = \int_{-\infty}^{\infty} y\, p(y)\,dy.
\]
\\
\noindent More generally, for any function $g(Y)$ we define
\[
  \mathbb{E}[g(Y)] = \int_{-\infty}^{\infty} g(y)\, p(y)\,dy.
\]
\\
\noindent In our setting, we'll consider a random vector
\[
  X = (X_1,\dots,X_d)
\]
\noindent that is \emph{uniformly distributed} on $[0,1]^d$. This means that every
region of $[0,1]^d$ with the same volume is equally likely. The
corresponding density $p(x)$ is constant on $[0,1]^d$ and zero outside. In
this special case, the expectation of $f(X)$ is
\[
  \mathbb{E}[f(X)] = \int_{[0,1]^d} f(x)\,dx = I(f,d).
\]
\\
\noindent Thus the integral $I(f,d)$ we wish to approximate is exactly the expected
value of $f(X)$ when $X$ is uniformly distributed on $[0,1]^d$.

\paragraph{Variance and standard deviation.}
The \emph{variance} of a random variable $Y$ is defined by
\[
  \Var(Y) = \mathbb{E}\bigl[(Y - \mathbb{E}[Y])^2\bigr],
\]
\noindent and measures the spread of $Y$ around its mean. The \emph{standard
deviation} is $\sqrt{\Var(Y)}$ and has the same units as $Y$. Intuitively,
the standard deviation is a measure of the typical size of the deviation
$Y - \mathbb{E}[Y]$.

\paragraph{Law of large numbers (LLN).}
Suppose $Y_1, Y_2, \dots$ are independent copies of the same random
variable $Y$ (we write $Y_i$ are i.i.d.). Consider the sample average
\[
  \overline{Y}_N = \frac{1}{N} \sum_{i=1}^N Y_i.
\]
\\
\noindent The \emph{law of large numbers} states that, under mild assumptions,
\[
  \overline{Y}_N \to \mathbb{E}[Y] \quad \text{as } N \to \infty,
\]
\noindent i.e., the sample average converges to the expected value. Intuitively, if
we take more and more samples, their average stabilizes near the true mean.
\\
\\
In our application, we'll take $Y_i = f(X_i)$, where $X_i$ are random
points in $[0,1]^d$ chosen independently and uniformly. The law of large
numbers then tells us that the average of the values $f(X_i)$ converges to
the integral $I(f,d)$.

\paragraph{Fluctuations and the $N^{-1/2}$ scaling.}
We are not only interested in convergence, but also in \emph{how fast} the
sample average approaches its mean. A key quantity is the fluctuation
\[
  \overline{Y}_N - \mathbb{E}[Y],
\]
\noindent which measures how far the sample average wanders away from the true mean
for a typical sample.
\\
\\
A simple variance calculation already reveals the $N^{-1/2}$ scaling. If
$\Var(Y) = \sigma^2 < \infty$ and $Y_i$ are i.i.d., then
\[
  \Var(\overline{Y}_N)
  = \Var\!\left(\frac{1}{N} \sum_{i=1}^N Y_i\right)
  = \frac{1}{N^2} \sum_{i=1}^N \Var(Y_i)
  = \frac{1}{N^2} \cdot N \sigma^2
  = \frac{\sigma^2}{N}.
\]
\\
\noindent Therefore the standard deviation of the sample average is
\[
  \sqrt{\Var(\overline{Y}_N)} = \frac{\sigma}{\sqrt{N}}.
\]
\\
\noindent Since the standard deviation is a measure of the typical size of
$\overline{Y}_N - \mathbb{E}[Y]$, we see that
\[
  \bigl|\overline{Y}_N - \mathbb{E}[Y]\bigr|
  \text{ is typically of size } O(N^{-1/2}).
\]

\paragraph{Central limit theorem (CLT).}
The \emph{central limit theorem} refines this by describing the shape of
the distribution of the error. Under mild conditions,
\[
  \sqrt{N}\,\bigl(\overline{Y}_N - \mathbb{E}[Y]\bigr)
  \Rightarrow \mathcal{N}(0,\sigma^2),
\]
\noindent where $\Rightarrow$ denotes convergence in distribution and
$\mathcal{N}(0,\sigma^2)$ is a normal (Gaussian) distribution with mean
zero and variance $\sigma^2$. This again shows that the natural scale of
the error $\overline{Y}_N - \mathbb{E}[Y]$ is $1/\sqrt{N}$, consistent
with the variance calculation above.
\\
\\
In the Monte Carlo setting we will take $Y_i = f(X_i)$, so the Monte Carlo
error is exactly a sample average minus a mean, and its typical size is of
order $N^{-1/2}$.

\subsection{Quadrature rules and Monte Carlo integration}
Any numerical integration method can be viewed as replacing the continuous
integral by a finite weighted sum of function values. More precisely, a
\emph{quadrature rule} with $N$ points has the form
\[
  Q_N(f) = \sum_{i=1}^N w_i\, f(x_i),
\]
\noindent where $x_i \in [0,1]^d$ are the nodes and $w_i \in \mathbb{R}$ are weights.
Classical deterministic quadrature rules (e.g., trapezoidal or Gaussian
quadrature in one dimension, and tensor-product versions in low
dimensions) correspond to carefully chosen deterministic nodes and weights.
\\
\\
Monte Carlo and quasi--Monte Carlo methods also fit this template, but
with different choices of nodes. In particular, Monte Carlo uses
\emph{random} nodes, while quasi--Monte Carlo uses \emph{deterministic}
nodes that are designed to be evenly spread over the integration domain.

\subsection{Plain Monte Carlo integration}
Plain Monte Carlo (MC) integration applies the ideas from the previous
subsection directly. We draw random points
\[
  X_1, \dots, X_N \sim \mathrm{Unif}([0,1]^d)
\]
\noindent independently and identically distributed, and approximate the integral by
the sample average
\[
  \widehat{I}_N^{\MC}(f,d)
  = \frac{1}{N} \sum_{i=1}^N f(X_i).
\]
\\
\noindent This is a quadrature rule of the form
\[
  Q_N(f) = \sum_{i=1}^N w_i f(x_i),
\]
\noindent with random nodes $x_i = X_i$ and equal weights $w_i = 1/N$.
\\
\\
Because $\mathbb{E}[f(X_i)] = I(f,d)$, the estimator is unbiased:
\[
  \mathbb{E}\bigl[\widehat{I}_N^{\MC}(f,d)\bigr] = I(f,d).
\]
\\
\noindent Moreover, the law of large numbers implies that
\[
  \widehat{I}_N^{\MC}(f,d) \to I(f,d)
  \qquad \text{as } N \to \infty,
\]
\noindent so the Monte Carlo approximation converges to the true integral when we
take more and more random points.
\\
\\
The variance calculation from the previous subsection can be applied with
$Y_i = f(X_i)$ and $\overline{Y}_N = \widehat{I}_N^{\MC}(f,d)$. If
$\Var(f(X)) = \sigma^2 < \infty$, then
\[
  \Var\bigl(\widehat{I}_N^{\MC}(f,d)\bigr)
  = \frac{\sigma^2}{N},
\]
\noindent and the standard deviation (a measure of the typical size of the error) is
\[
  \sqrt{\Var\bigl(\widehat{I}_N^{\MC}(f,d)\bigr)}
  = \frac{\sigma}{\sqrt{N}}.
\]
\\
\noindent Thus the Monte Carlo error
\[
  \widehat{I}_N^{\MC}(f,d) - I(f,d)
\]
\noindent has typical magnitude of order $N^{-1/2}$. In the language of big-$O$
notation, the root mean square error (RMSE) satisfies
\[
  \sqrt{\mathbb{E}\bigl[(\widehat{I}_N^{\MC} - I)^2\bigr]}
  = O(N^{-1/2}),
\]
\noindent with a constant that depends on the variance of $f(X)$ and may grow with
the dimension $d$, but the exponent $-1/2$ itself does not depend on $d$.
This $N^{-1/2}$ scaling is a hallmark of Monte Carlo methods.

\subsection{Quasi-Monte Carlo with Sobol sequences}
Quasi-Monte Carlo (QMC) methods replace the random nodes of Monte Carlo
with a deterministic sequence of points that is designed to fill the unit
cube $[0,1]^d$ as uniformly as possible. Let $(x_i)_{i \ge 1}$ be a
\emph{low-discrepancy sequence} in $[0,1]^d$, such as a Sobol sequence. The
corresponding QMC estimator is
\[
  \widehat{I}_N^{\QMC}(f,d)
  = \frac{1}{N} \sum_{i=1}^N f(x_i).
\]
\\
\noindent This again has the quadrature form $Q_N(f) = \frac{1}{N}\sum f(x_i)$, but
now the nodes $x_i$ are carefully constructed rather than random.
\\
\\
The key quantity in QMC analysis is the (star) discrepancy
$D^*(x_1,\dots,x_N)$ of the first $N$ points, which measures how far the
empirical distribution of the points deviates from the uniform
distribution on $[0,1]^d$. Lower discrepancy means the points are more
uniformly spread out.
\\
\\
For functions of bounded variation in the sense of Hardy--Krause, the
Koksma-Hlawka inequality states that
\[
  \bigl|\widehat{I}_N^{\QMC}(f,d) - I(f,d)\bigr|
  \leq V_{\mathrm{HK}}(f)\, D^*(x_1,\dots,x_N),
\]
\noindent where $V_{\mathrm{HK}}(f)$ is the (Hardy-Krause) variation of $f$. For
many low-discrepancy sequences (including Sobol), the discrepancy
satisfies
\[
  D^*(x_1,\dots,x_N)
  = O\!\left(\frac{(\log N)^d}{N}\right)
  \qquad \text{as } N \to \infty.
\]
\\
\noindent Ignoring the logarithmic factor $(\log N)^d$, this suggests that QMC can
achieve an error that behaves like $O(N^{-1})$ for sufficiently smooth
integrands with ``low effective dimension,'' which is asymptotically better
than the Monte Carlo rate $O(N^{-1/2})$.
\\
\\
In practice, QMC methods often show substantially smaller errors than MC
for the same $N$ on smooth, well-behaved integrands, especially in
moderate dimensions. When the dimension is very high or the integrand has
significant discontinuities or strong localized features, the advantage of
QMC can diminish.

\subsection{Error metrics and comparison criteria}
For any method (MC or QMC) that produces an approximation
$\widehat{I}_N(f,d)$ to the integral $I(f,d)$, we define the absolute
error
\[
  E_N(f,d) = \bigl|\widehat{I}_N(f,d) - I(f,d)\bigr|,
\]
\noindent where $I(f,d)$ is the exact integral, known in closed form for our chosen
test functions. We will compare MC and QMC using three related criteria:
\begin{enumerate}
  \item \textbf{Error versus sample size:} we will plot $E_N(f,d)$ versus
        $N$ on logarithmic scales and estimate empirical convergence rates
        by fitting a slope to $\log_{10}(E_N)$ versus $\log_{10}(N)$ for
        each method.
  \item \textbf{Error versus wall-clock time:} we will record the
        computation time required to obtain each approximation and compare
        time-to-accuracy by plotting $E_N(f,d)$ against wall-clock time.
  \item \textbf{Variance and stability:} when repeated runs are performed
        (for Monte Carlo and, optionally, randomized QMC), we will
        summarize the distribution of errors (for example, via mean and
        standard deviation) to assess the stability of the estimators.
\end{enumerate}
\noindent This framework allows us to quantify both the asymptotic behavior
(convergence rates) and the practical performance (time-to-accuracy) of
plain Monte Carlo and Sobol quasi-Monte Carlo integration on
high-dimensional test problems.


% =====================================================
\section{Test Integrands and Experimental Setup}
\label{sec:setup}
In this section we describe the specific test problems and experimental
parameters used to compare Monte Carlo and quasi--Monte Carlo integration
on high-dimensional integrals over $[0,1]^d$.

\subsection{Dimensions and general problem statement}

For each experiment we fix a dimension
\[
  d \in \{5, 10, 15, 20\},
\]
\noindent and consider integrals of the form
\[
  I(f,d) = \int_{[0,1]^d} f(x)\,dx,
  \qquad x = (x_1,\dots,x_d) \in [0,1]^d.
\]
\\
\noindent The goal is to approximate $I(f,d)$ using both plain Monte Carlo (MC) and
Sobol quasi--Monte Carlo (QMC), and to measure the resulting errors and
computation times.

\subsection{Test integrands with known exact values}
To evaluate the accuracy of MC and QMC, we choose test functions
$f : [0,1]^d \to \mathbb{R}$ for which the integral $I(f,d)$ can be computed
in closed form (possibly involving special functions). This allows us to
compute the \emph{true error} $\bigl|\widehat{I}_N(f,d) - I(f,d)\bigr|$ for
each method and each configuration.
\\
\\
The first three test integrands we use, $f_A$, $f_B$, and $f_C$, are
\emph{separable} in the sense that they factor into a product of
one-dimensional functions,
\[
  f(x) = \prod_{j=1}^d g_j(x_j),
\]
so that the $d$-dimensional integral reduces to a product of
one-dimensional integrals,
\[
  I(f,d) = \int_{[0,1]^d} f(x)\,dx
         = \prod_{j=1}^d \int_0^1 g_j(x_j)\,dx_j.
\]
This structure makes it straightforward to obtain exact values while still
providing a variety of behaviors for the numerical methods.
\\
\\
We work with four test integrands in total, denoted $f_A$, $f_B$, $f_C$,
and $f_D$.

\paragraph{Integrand A (separable exponential).}
Our first test integrand is a symmetric exponential:
\[
  f_A(x) = \exp\!\Bigl(-\sum_{j=1}^d x_j\Bigr)
         = \prod_{j=1}^d e^{-x_j}.
\]
The exact integral is
\[
  I(f_A,d)
  = \int_{[0,1]^d} f_A(x)\,dx
  = \prod_{j=1}^d \int_0^1 e^{-x_j}\,dx_j
  = \prod_{j=1}^d \bigl(1 - e^{-1}\bigr)
  = \bigl(1 - e^{-1}\bigr)^d.
\]
This integrand is smooth, bounded, and treats all coordinates
symmetrically.

\paragraph{Integrand B (rational integrand with arctan closed form).}
The second test integrand is a smooth rational function. For
$j = 1,\dots,d$ define
\[
  g_j(x_j) = \frac{1}{1 + j x_j^2},
\]
and set
\[
  f_B(x) = \prod_{j=1}^d g_j(x_j)
         = \prod_{j=1}^d \frac{1}{1 + j x_j^2}.
\]
Each one-dimensional factor has an explicit antiderivative:
\[
  \int_0^1 \frac{1}{1 + j x_j^2}\,dx_j
  = \frac{1}{\sqrt{j}} \arctan\bigl(\sqrt{j}\,x_j\bigr)\Big|_{x_j=0}^{x_j=1}
  = \frac{1}{\sqrt{j}} \arctan\bigl(\sqrt{j}\bigr),
\]
so that
\[
  I(f_B,d)
  = \prod_{j=1}^d \frac{1}{\sqrt{j}} \arctan\bigl(\sqrt{j}\bigr).
\]
This integrand is smooth but has a different shape from the exponential
example: it is close to $1$ near $x_j = 0$ and decays as $x_j$ increases,
with the rate depending on $j$. The dependence on $j$ introduces a form of
anisotropy, since directions corresponding to larger $j$ are damped more
strongly.

\paragraph{Integrand C (Gaussian-type integrand).}
The third test integrand is inspired by the Gaussian (normal) distribution.
We define
\[
  f_C(x) = \exp\!\Bigl(-\sum_{j=1}^d x_j^2\Bigr)
         = \prod_{j=1}^d e^{-x_j^2}.
\]
The corresponding one-dimensional integral
\[
  \int_0^1 e^{-x^2}\,dx
\]
does not have an elementary antiderivative, but it is a classical integral
that can be expressed in terms of the \emph{error function}, defined by
\[
  \operatorname{erf}(z)
  = \frac{2}{\sqrt{\pi}} \int_0^z e^{-t^2}\,dt.
\]
From this definition we obtain
\[
  \int_0^1 e^{-x^2}\,dx
  = \frac{\sqrt{\pi}}{2}\,\operatorname{erf}(1).
\]
Therefore, by separability,
\[
  I(f_C,d)
  = \int_{[0,1]^d} f_C(x)\,dx
  = \prod_{j=1}^d \int_0^1 e^{-x_j^2}\,dx_j
  = \left(\int_0^1 e^{-x^2}\,dx\right)^d
  = \left(\frac{\sqrt{\pi}}{2}\,\operatorname{erf}(1)\right)^d.
\]
This integrand is smooth and strongly related to the Gaussian integral
that underlies the normal distribution, providing a natural probabilistic
connection in our test set.

\paragraph{Integrand D (low effective dimension).}
To include an example that is especially favorable to
quasi--Monte Carlo, we define
\[
  f_D(x) = \left(\frac{1}{5}\sum_{j=1}^5 x_j\right)^2,
  \qquad x \in [0,1]^d,\ d \ge 5.
\]
This integrand is smooth and depends only on the first five coordinates,
so it has low \emph{effective} dimension even when the nominal dimension
$d$ is large. Let $X_1,\dots,X_5$ be independent $\mathrm{Unif}(0,1)$
random variables and set
\[
  S = X_1 + \cdots + X_5.
\]
Then
\[
  I(f_D,d)
  = \mathbb{E}\!\left[\left(\frac{S}{5}\right)^2\right]
  = \frac{1}{25}\,\mathbb{E}[S^2]
  = \frac{1}{25}\Bigl(\mathrm{Var}(S) + (\mathbb{E}S)^2\Bigr).
\]
Since $\mathbb{E}X_i = \tfrac{1}{2}$ and $\mathrm{Var}(X_i) = \tfrac{1}{12}$,
we have
\[
  \mathbb{E}S = 5 \cdot \frac{1}{2} = \frac{5}{2},
  \qquad
  \mathrm{Var}(S) = 5 \cdot \frac{1}{12} = \frac{5}{12},
\]
and hence
\[
  \mathbb{E}[S^2]
  = \mathrm{Var}(S) + (\mathbb{E}S)^2
  = \frac{5}{12} + \left(\frac{5}{2}\right)^2
  = \frac{20}{3}.
\]
Therefore
\[
  I(f_D,d) = \frac{1}{25} \cdot \frac{20}{3} = \frac{4}{15}.
\]
Thus for any $d \ge 5$ we have
\[
  I(f_D,d) = \frac{4}{15},
\]
independent of the nominal dimension $d$.
\\
\\
Together, $f_A$, $f_B$, $f_C$, and $f_D$ provide a small but diverse
collection of high-dimensional test problems. The first three are smooth
and separable but differ in symmetry, anisotropy, and decay behavior,
while $f_D$ is a smooth polynomial with low effective dimension, designed
to highlight the setting where quasi--Monte Carlo is particularly
effective.

\subsection{Sample sizes and repetitions}
For each integrand $f \in \{f_A,f_B,f_C,f_D\}$, each dimension
$d \in \{5,10,15,20\}$, and each numerical method (MC or QMC), we consider
a sequence of sample sizes
\[
  N \in \{2^7, 2^8, \dots, 2^{15}\}
    = \{128, 256, 512, \dots, 32768\}.
\]
This range allows us to observe the convergence behavior over several
orders of magnitude in $N$.

For \emph{plain Monte Carlo}, the estimator
$\widehat{I}_N^{\MC}(f,d)$ is random, so we perform
\[
  R = 30
\]
independent runs for each choice of $(f,d,N)$. This yields an ensemble of
errors, from which we can compute statistics such as the mean error and
standard deviation.

For \emph{Sobol quasi--Monte Carlo}, the estimator
$\widehat{I}_N^{\QMC}(f,d)$ is deterministic once $f$, $d$, and $N$ are
fixed, so in the simplest version we perform a single run for each
configuration. Our framework also allows for \emph{randomized} Sobol
sequences (scrambled QMC), in which case we could again take $R$ repeated
runs to estimate a variance; however, in this project we primarily focus
on the deterministic Sobol case.

\subsection{Error computation}
For each method, integrand, dimension, and sample size we obtain an
approximation $\widehat{I}_N(f,d)$ to the exact integral $I(f,d)$. The
\emph{absolute error} is defined by
\[
  E_N(f,d) = \bigl|\widehat{I}_N(f,d) - I(f,d)\bigr|.
\]

In the Monte Carlo case, the estimator $\widehat{I}_N^{\MC}(f,d)$ is
random, so repeated runs with different random seeds produce different
approximations and hence different errors. For a fixed configuration
$(f,d,N)$ we perform $R$ independent runs, yielding a collection of error
values
\[
  E_N^{(1)}(f,d),\; E_N^{(2)}(f,d),\; \dots,\;
  E_N^{(R)}(f,d).
\]
It is natural to think of these as $R$ samples from an underlying random
variable ``error'' at sample size $N$.

From these $R$ samples we compute two basic summary statistics:
\begin{itemize}
  \item the \emph{sample mean error}
        \[
          \overline{E}_N(f,d)
          = \frac{1}{R} \sum_{r=1}^R E_N^{(r)}(f,d),
        \]
        which is simply the average of the observed errors and serves as
        an estimate of the typical error size at that $N$;

  \item the \emph{sample standard deviation} of the error
        \[
          \mathrm{std}_N(f,d)
          = \sqrt{\frac{1}{R-1} \sum_{r=1}^R
              \bigl(E_N^{(r)}(f,d) - \overline{E}_N(f,d)\bigr)^2}.
        \]
\end{itemize}
The quantity inside the square root is the \emph{sample variance} with
denominator $R-1$ rather than $R$; this choice (sometimes called the
Bessel correction) makes the sample variance an unbiased estimator of the
true variance of the error when the $E_N^{(r)}$ are independent samples.
The sample standard deviation $\mathrm{std}_N(f,d)$ therefore provides an
estimate of the typical fluctuation of the Monte Carlo error around its
mean.

For deterministic QMC, the estimator $\widehat{I}_N^{\QMC}(f,d)$ (and
hence the error $E_N(f,d)$) is fully determined by $f$, $d$, and $N$, so
we simply report the single absolute error for each configuration.

\subsection{Timing measurements}
In addition to accuracy, we are interested in the practical cost of each
method. For each run we measure the wall-clock time required to
\emph{generate the sample points} and \emph{evaluate the integrand} on
those points. We denote this time by $T_N(f,d)$.

By plotting the error $E_N(f,d)$ against the corresponding time
$T_N(f,d)$, we obtain \emph{time-to-accuracy} plots that show, for
example, which method reaches a target error (e.g.\ $10^{-3}$) in less
time for a given integrand and dimension.

\subsection{Summary of comparison metrics}
Summarizing, for each integrand $f$, dimension $d$, method (MC or QMC),
and sample size $N$, we collect:
\begin{itemize}
  \item absolute errors $E_N(f,d)$ (and, for MC, averages and standard
        deviations over repeated runs),
  \item corresponding wall-clock times $T_N(f,d)$.
\end{itemize}
We will use these data to construct:
\begin{enumerate}
  \item log--log plots of $E_N(f,d)$ versus $N$ to estimate empirical
        convergence rates for MC and QMC,
  \item plots of $E_N(f,d)$ versus $T_N(f,d)$ to compare time-to-accuracy,
\end{enumerate}
These diagnostics allow us to evaluate both the asymptotic behavior and
the practical performance of plain Monte Carlo and Sobol quasi--Monte
Carlo integration on the chosen high-dimensional test problems.



% =====================================================
% =====================================================
\section{Numerical Results}
\label{sec:results}

In this section we present the numerical comparison between plain Monte
Carlo and Sobol quasi--Monte Carlo for the four test integrands
$f_A, f_B, f_C$, and $f_D$ in dimensions
$d \in \{5,10,15,20\}$. For each configuration $(f,d)$ we consider
sample sizes
\[
  N \in \{2^7,2^8,\dots,2^{15}\}
  = \{128,256,\dots,32768\},
\]
perform $R=30$ independent runs for Monte Carlo, and use a single Sobol
net of size $N=2^m$ for QMC (via \texttt{random\_base2} with
$N = 2^m$). We report absolute errors
$E_N(f,d) = |\widehat{I}_N(f,d) - I(f,d)|$ and corresponding
wall-clock times for each method.

\subsection{Error versus sample size}

Figures~\ref{fig:errorN_A_d5}--\ref{fig:errorN_D_d20} show representative
log--log plots of error versus sample size $N$ for selected integrands
and dimensions. In each plot, the blue curve corresponds to the Monte
Carlo mean error over $R=30$ runs, and the orange curve corresponds to
the single Sobol QMC error.

\paragraph{Integrands $f_A$ and $f_B$ in moderate dimension.}
For $d=5$, both $f_A$ and $f_B$ display the ``textbook'' quasi--Monte
Carlo behavior. As a representative example, Figure~\ref{fig:errorN_A_d5}
shows the error for $f_A$ in $d=5$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{\figpath error_vs_N_integrand_A_d5.png}
  \caption{Error versus sample size on a log--log scale for
  integrand $f_A$ in dimension $d=5$.}
  \label{fig:errorN_A_d5}
\end{figure}

In this case the Sobol QMC curve lies below the MC mean error curve for
moderate and large $N$. The QMC error decreases roughly linearly in the
log--log plot with slope close to $-1$, while the MC error decreases
more slowly, with a slope closer to $-1/2$ and noticeably more noise.
This is consistent with the theoretical rates
$O(N^{-1})$ for QMC (up to logarithmic factors) and $O(N^{-1/2})$ for MC
for smooth, well-behaved integrands.

A similar picture is observed for $f_B$ in $d=5$ (not shown): QMC again
produces smaller errors than MC for large $N$, with a nearly straight
line of slope close to $-1$ on the log--log plot.

\paragraph{Integrands $f_A$ and $f_B$ in high dimension.}
The situation changes markedly when we increase the dimension to $d=20$.
Figures~\ref{fig:errorN_A_d20} and~\ref{fig:errorN_B_d20} show the error
versus $N$ for $f_A$ and $f_B$ in $d=20$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{\figpath error_vs_N_integrand_A_d20.png}
  \caption{Error versus sample size on a log--log scale for
  integrand $f_A$ in dimension $d=20$.}
  \label{fig:errorN_A_d20}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{\figpath error_vs_N_integrand_B_d20.png}
  \caption{Error versus sample size on a log--log scale for
  integrand $f_B$ in dimension $d=20$.}
  \label{fig:errorN_B_d20}
\end{figure}

For both $f_A$ and $f_B$ in $d=20$, the QMC error still decays roughly
like $O(N^{-1})$ and appears as an almost straight line of slope close
to $-1$ on the log--log plots. However, the QMC errors start at a
relatively large value and remain significantly above the MC mean errors
throughout the range of $N$ considered. In contrast, the MC error curves
start at much smaller values and decay more slowly, behaving like
$O(N^{-1/2})$, but nevertheless stay below the corresponding QMC errors
for all $N$ in our experiments.

This illustrates an important point: while QMC enjoys a better
\emph{asymptotic} rate, the multiplicative constants in the bounds can
be large, especially for high-dimensional integrals whose exact values
(and variances) are extremely small. For $f_A$ and $f_B$ in $d=20$, the
true integrals are on the order of $10^{-4}$ and $10^{-8}$, respectively.
The MC errors inherit this small scale through the variance, whereas the
QMC error behaves more like $C/N$ with a constant $C$ of order one. Over
the practical range of $N$ used here, MC therefore outperforms QMC.

\paragraph{Integrand $f_C$: pre-asymptotic MC, asymptotic QMC.}
For the Gaussian-type integrand $f_C$, the behavior in $d=20$ lies
between the previous two extremes. Figure~\ref{fig:errorN_C_d20} shows
the error versus $N$ for $f_C$ in $d=20$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{\figpath error_vs_N_integrand_C_d20.png}
  \caption{Error versus sample size on a log--log scale for
  integrand $f_C$ in dimension $d=20$.}
  \label{fig:errorN_C_d20}
\end{figure}

At smaller sample sizes, the MC mean error lies below the QMC error: MC
performs better in this ``pre-asymptotic'' regime. As $N$ increases, the
QMC error decreases more rapidly, with a slope closer to $-1$ than
$-1/2$, and eventually the QMC curve meets and slightly undercuts the MC
curve at the largest $N$ values. This example shows a regime where the
superior asymptotic rate of QMC becomes visible, but only after a range
of $N$ in which MC gives smaller errors.

\paragraph{Integrand $f_D$: low effective dimension and clear QMC win.}
Finally, we consider the low effective-dimension integrand $f_D$. Recall
that $f_D$ depends only on the first five coordinates, even when the
nominal dimension $d$ is much larger, and that
$I(f_D,d) = 4/15$ for all $d \ge 5$. Figures~\ref{fig:errorN_D_d5}
and~\ref{fig:errorN_D_d20} show the error versus $N$ for $d=5$ and
$d=20$, respectively.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{\figpath error_vs_N_integrand_D_d5.png}
  \caption{Error versus sample size on a log--log scale for
  integrand $f_D$ in dimension $d=5$.}
  \label{fig:errorN_D_d5}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{\figpath error_vs_N_integrand_D_d20.png}
  \caption{Error versus sample size on a log--log scale for
  integrand $f_D$ in dimension $d=20$.}
  \label{fig:errorN_D_d20}
\end{figure}

For $f_D$, Sobol QMC clearly outperforms MC in both dimensions. The QMC
error curve lies below the MC curve for all $N$ considered and is almost
perfectly linear on the log--log scale with a slope close to $-1$. The
MC mean error exhibits the expected $O(N^{-1/2})$ decay with more
variability and remains roughly an order of magnitude larger than the
QMC error at the largest $N$. Furthermore, the QMC error curves for
$d=5$ and $d=20$ are very similar, reflecting the fact that the
\emph{effective} dimension of the problem is five regardless of the
nominal $d$. This example confirms the classical theory: when the
integrand is smooth and depends primarily on a small number of
coordinates, quasi--Monte Carlo can deliver the expected $O(N^{-1})$-type
behavior and a substantial accuracy advantage over plain Monte Carlo.

\subsection{Error versus time (time-to-accuracy)}

The error-versus-time plots (not all shown) exhibit patterns that closely
mirror the error-versus-$N$ plots. For each integrand and dimension, the
MC and QMC curves in the $(T_N, E_N)$ plane track the corresponding
$(N,E_N)$ behavior, since the cost per sample is similar for both
methods. In particular:
\begin{itemize}
  \item For $f_A$ and $f_B$ in $d=5$, QMC reaches a given target error
        (e.g.\ $10^{-3}$ or $10^{-4}$) in less wall-clock time than MC.
  \item For $f_A$ and $f_B$ in $d=20$, MC not only attains smaller errors
        for a given $N$, but also reaches any fixed error tolerance
        faster than QMC over the range of $N$ studied.
  \item For $f_C$ in $d=20$, MC is initially more efficient, but QMC
        catches up and slightly overtakes MC at the largest $N$ values.
  \item For the low effective-dimension integrand $f_D$, QMC is uniformly
        better than MC both as a function of $N$ and as a function of
        wall-clock time: it reaches any target error much more quickly.
\end{itemize}

Overall, the time-to-accuracy plots confirm that the qualitative
comparisons observed in the error-versus-$N$ plots translate directly
into practical computational efficiency.



% =====================================================
\section{Discussion and Conclusions}
\label{sec:conclusion}

This project compared plain Monte Carlo (MC) and Sobol quasi--Monte Carlo
(QMC) for approximating high-dimensional integrals over $[0,1]^d$ for a
set of smooth test integrands. The numerical experiments were designed to
illustrate both the classical theoretical picture---$O(N^{-1/2})$ error
for MC versus a potential $O(N^{-1})$ behavior for QMC---and some of the
subtleties that arise in practice, especially in higher dimension.

\subsection{Summary of numerical findings}

Broadly speaking, the experiments support the following observations:
\begin{itemize}
  \item In moderate dimension ($d=5$), Sobol QMC behaves very much in line
        with the standard theory on all three separable integrands
        $f_A,f_B,f_C$. For a fixed number of samples $N$, the QMC error
        is smaller than the Monte Carlo mean error, and the log--log
        error-versus-$N$ plots for QMC show an almost straight line with
        slope close to $-1$, while the MC curves decay more slowly with
        slope closer to $-1/2$ and display more variability.
  \item In higher dimension ($d=20$), the picture becomes more nuanced.
        For $f_A$ (the separable exponential) and $f_B$ (the rational
        integrand), MC actually outperforms QMC over the entire range of
        sample sizes considered. The QMC error still decreases roughly
        like $O(N^{-1})$, but it starts from a much larger value and
        remains above the MC mean error for all $N \le 32768$.
  \item For the Gaussian-type integrand $f_C$ in $d=20$, MC has smaller
        errors at small $N$, while QMC decays faster and eventually
        catches up and slightly surpasses MC at the largest $N$ values.
        This example shows a transition from a ``pre-asymptotic'' regime,
        where MC is more accurate, to a regime where the asymptotic
        advantage of QMC starts to appear.
  \item For the low effective-dimension integrand $f_D$, Sobol QMC
        clearly and consistently outperforms MC in both $d=5$ and
        $d=20$. The QMC error curves for $f_D$ are nearly identical in
        $d=5$ and $d=20$, reflecting the fact that $f_D$ depends only on
        the first five coordinates, while the MC errors are larger and
        exhibit the expected $O(N^{-1/2})$ decay with more noise.
\end{itemize}
The error-versus-time plots mirror these conclusions: whenever QMC
achieves smaller errors for a given $N$, it also reaches a specified
target tolerance in less wall-clock time, and conversely in the cases
where MC is more accurate.

\subsection{Rates, constants, and effective dimension}

One of the main lessons from these experiments is that the asymptotic
rates $O(N^{-1/2})$ for MC and $O(N^{-1})$ (up to logarithmic factors) for
QMC are only part of the story. The multiplicative constants and the
\emph{effective} dimension of the integrand play a crucial role.

For $f_A$ and $f_B$ in $d=20$, the exact integrals $I(f_A,20)$ and
$I(f_B,20)$ are extremely small (on the order of $10^{-4}$ and $10^{-8}$,
respectively), and the variance of $f(X)$ under a uniform random
$X \in [0,1]^d$ is correspondingly tiny. As a result, the MC error
$\widehat{I}_N^{\MC} - I$ naturally lives on this small scale and is
roughly $\sigma/\sqrt{N}$ with $\sigma \ll 1$. In contrast, the QMC error
behaves more like $C/N$ with a constant $C$ that is effectively of order
one. Over the range $N \le 32768$, the advantage of the $1/N$ rate is not
enough to overcome the unfavorable constant, so MC remains more accurate
than QMC for these particular integrands and dimensions.

The integrand $f_C$ illustrates an intermediate situation: the constants
are such that MC is better at small $N$, but QMC eventually catches up as
$N$ increases and the $1/N$ decay begins to dominate. In practice, this
means that whether QMC is beneficial depends not only on the smoothness
of $f$, but also on the range of sample sizes one is willing or able to
use.

The low effective-dimension example $f_D$ highlights the setting in which
QMC performs in a ``textbook'' fashion. Although the nominal dimension is
as large as $d=20$, $f_D$ only depends on the first five coordinates, and
the remaining coordinates are irrelevant. In this situation, the Sobol
sequence can effectively exploit its low-discrepancy structure in a
five-dimensional space, and the error-versus-$N$ curves for $d=5$ and
$d=20$ are nearly identical. MC still converges at the universal
$N^{-1/2}$ rate, but its errors are uniformly larger than those of QMC
for all $N$ in the experiment. This example confirms the theoretical
insight that QMC is especially effective when the integrand has low
effective dimension or when most of the variation is concentrated in the
first few coordinates.

\subsection{Practical takeaways}

From a practical point of view, the experiments suggest the following
guidelines:
\begin{itemize}
  \item For smooth, moderate-dimensional integrals with reasonably sized
        integrals and variances, Sobol QMC can provide a clear accuracy
        and efficiency advantage over plain Monte Carlo.
  \item In very high dimensions, or for integrands whose exact values and
        variances are extremely small, MC may outperform QMC over the
        range of sample sizes that are computationally affordable. In
        these cases, the constants hidden in the $O(N^{-1})$ and
        $O(N^{-1/2})$ notations are important.
  \item When there is evidence that the integrand has low effective
        dimension (for example, the integrand mainly depends on a few
        coordinates or principal components), QMC is particularly
        attractive and often displays the expected $N^{-1}$-type
        behavior even when the nominal dimension is large.
\end{itemize}

Overall, the experiments in this project show that quasi--Monte Carlo can
indeed achieve the improved convergence behavior suggested by theory in
favorable cases, but they also highlight that in high-dimensional,
small-variance settings, plain Monte Carlo can be competitive or even
superior over realistic ranges of sample sizes. The choice between MC and
QMC (or a combination of both) should therefore be informed by the
structure and effective dimension of the particular problem at hand.


% =====================================================
\section{Works Cited}

% =====================================================
\begin{thebibliography}{9}

\bibitem{niederreiter}
H.~Niederreiter,
\emph{Random Number Generation and Quasi--Monte Carlo Methods},
SIAM, 1992.

\bibitem{lemieux}
C.~Lemieux,
\emph{Monte Carlo and Quasi--Monte Carlo Sampling},
Springer, 2009.

\bibitem{quarteroni}
A.~Quarteroni, R.~Sacco, and F.~Saleri,
\emph{Numerical Mathematics},
2nd ed., Springer, 2007.

\bibitem{scipy-sobol}
SciPy Developers,
\emph{\texttt{scipy.stats.qmc.Sobol} documentation},
SciPy Reference Guide,
\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Sobol.html},
accessed November~2025.

\bibitem{owen}
A.~B.~Owen,
\emph{Monte Carlo Theory, Methods and Examples},
2013.
Available online at \url{https://artowen.su.domains/mc/}.

\end{thebibliography}

% =====================================================
\appendix

\section{Python Code and Data Files}
\label{sec:code}

All numerical experiments were implemented in Python using the scripts in
the \texttt{Code} subdirectory of the project. The main components are:
\begin{itemize}
  \item \texttt{integrands.py}: definitions of the test integrands
        $f_A, f_B, f_C, f_D$ and their exact integrals $I(f,d)$.
  \item \texttt{mc\_qmc.py}: implementations of plain Monte Carlo and
        Sobol quasi--Monte Carlo estimators on $[0,1]^d$.
  \item \texttt{run\_experiments.py}: driver script that runs all
        experiments over $f \in \{f_A,f_B,f_C,f_D\}$,
        $d \in \{5,10,15,20\}$, and
        $N \in \{2^7,\dots,2^{15}\}$, and writes the aggregated data
        to \texttt{results\_mc\_qmc.csv}.
  \item \texttt{plot\_results.py}: script that reads
        \texttt{results\_mc\_qmc.csv} and generates the log--log error
        versus $N$ and error versus time plots shown in
        Section~\ref{sec:results}.
\end{itemize}

The file \texttt{results\_mc\_qmc.csv} contains the numerical data used
to produce all figures in this report.

A complete copy of the project (Python code, data files, and \LaTeX{}
sources for this report) is available at
\url{https://github.com/yourusername/MATH_505_Final_Project}.



\end{document}
