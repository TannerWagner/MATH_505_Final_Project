\documentclass{beamer}

\usetheme{Madrid}

% ---------- Packages ----------
\usepackage{amsmath, amssymb, amsthm, mathtools}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}

\definecolor{linkblue}{RGB}{0,0,140}
\hypersetup{
  colorlinks = true,
  linkcolor  = linkblue,
  citecolor  = linkblue,
  urlcolor   = linkblue
}

% ---------- Macros ----------
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\MC}{\mathrm{MC}}
\newcommand{\QMC}{\mathrm{QMC}}
\newcommand{\highlight}[1]{\boxed{#1}}
\newcommand{\figpath}{../Code/} % or just {} if you prefer

% ---------- Title metadata (used only in PDF info, not for slide) ----------
\title[QMC vs MC]{Quasi-Monte Carlo vs Monte Carlo\\[0.25em]
for High-Dimensional Integration on $[0,1]^d$}
\author[T. Wagner]{Tanner Wagner \\[0.25em]
MATH 505: Numerical Analysis \\[0.25em]
University of New Mexico}
\date{\today}

% ---------- Document begins ----------
\begin{document}

% ---------- Title slide ----------
\begin{frame}[plain]
  \centering
  \vspace{1.5cm}

  {\Huge Quasi-Monte Carlo vs Monte Carlo\par}
  \vspace{0.4cm}
  {\Large for High-Dimensional Integration on $[0,1]^d$\par}

  \vspace{1.5cm}
  {\Large Tanner Wagner\par}
  \vspace{0.25cm}
  {\large MATH 505: Numerical Analysis\par}
  \vspace{0.25cm}
  {\large University of New Mexico\par}

  \vfill
  {\large \today\par}
\end{frame}

% ---------- Outline slide ----------
\begin{frame}{Outline}
  \tableofcontents
\end{frame}



% =====================================================
\section{Motivation and Goal}

\begin{frame}{High-dimensional integrals}
  \begin{itemize}
    \item Many applications involve integrals over $[0,1]^d$:
      \begin{itemize}
        \item expectations in probability and statistics,
        \item option pricing in mathematical finance,
        \item posterior averages in Bayesian inference.
      \end{itemize}
    \item Classical tensor-product quadrature (Gaussian, Newton--Cotes):
      \begin{itemize}
        \item work grows exponentially with $d$,
        \item the \emph{curse of dimensionality}.
      \end{itemize}
    \item Need methods whose cost scales more gently with dimension.
  \end{itemize}
\end{frame}

\begin{frame}{Goal of the project}
  \begin{itemize}
    \item Compare \textbf{plain Monte Carlo (MC)} with
      \textbf{Sobol quasi--Monte Carlo (QMC)} for integrals
      \[
        I(f,d) = \int_{[0,1]^d} f(x)\,dx, \quad d \in \{5,10,15,20\}.
      \]
    \item Test on several smooth, high-dimensional integrands with
      known exact values.
    \item Metrics:
      \begin{itemize}
        \item absolute error,
        \item empirical convergence rate,
        \item \textbf{time-to-accuracy}.
      \end{itemize}
    \item Question: \emph{When does QMC actually beat MC in practice?}
  \end{itemize}
\end{frame}

% =====================================================
\section{Background}

\begin{frame}{Random variables in this talk}
  \begin{itemize}
    \item Think of a \textbf{random variable} as:
      \begin{itemize}
        \item a rule that assigns a number to the outcome of an experiment,
        \item e.g.\ roll a die $\to$ number $1,\dots,6$.
      \end{itemize}
    \item In our setting:
      \begin{itemize}
        \item $X = (X_1,\dots,X_d)$ is a random point in $[0,1]^d$,
        \item you can picture it as throwing a dart uniformly at the unit cube.
      \end{itemize}
    \item A function value $f(X)$ is then also a random variable
          (it depends on where the dart lands).
  \end{itemize}
\end{frame}

\begin{frame}{i.i.d.\ samples and a coin-flip analogy}
  \begin{itemize}
    \item \textbf{i.i.d.} = independent and identically distributed:
      \begin{itemize}
        \item each sample is drawn with the same distribution,
        \item and no sample influences any other.
      \end{itemize}
    \item Coin-flip analogy:
      \begin{itemize}
        \item flip a fair coin $N$ times,
        \item the fraction of heads is a sample average of i.i.d.\ Bernoulli r.v.'s.
      \end{itemize}
    \item We do the same thing with integrals:
      \begin{itemize}
        \item draw $X_1,\dots,X_N$ i.i.d.\ uniform on $[0,1]^d$,
        \item look at the sample average of $f(X_i)$.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Integral as an expectation}
  \begin{itemize}
    \item Let $X = (X_1,\dots,X_d)$ be uniform on $[0,1]^d$.
    \item ``Uniform'' here means every region of $[0,1]^d$ with the same volume
          is equally likely.
    \item Then
      \[
        I(f,d) = \int_{[0,1]^d} f(x)\,dx = \E[f(X)].
      \]
    \item Approximate $I(f,d)$ by a sample mean:
      \[
        \widehat{I}_N = \frac{1}{N}\sum_{i=1}^N f(X_i),
      \]
      with $X_1,\dots,X_N$ i.i.d.\ $\mathrm{Unif}([0,1]^d)$.
    \item This is the basic idea of Monte Carlo integration:
          \emph{“average many samples to approximate the true average.”}
  \end{itemize}
\end{frame}

\begin{frame}{Why Monte Carlo works (high-level picture)}
  \begin{itemize}
    \item \textbf{Law of Large Numbers (LLN):}
      \begin{itemize}
        \item sample average $\widehat{I}_N$ converges to the true mean $\E[f(X)]$
              as $N \to \infty$,
        \item just like the fraction of heads $\to \tfrac{1}{2}$ for a fair coin.
      \end{itemize}
    \item Intuition:
      \begin{itemize}
        \item each new sample adds a little information about the true average,
        \item as $N$ grows, the ``random wiggles'' cancel out.
      \end{itemize}
    \item \textbf{Central Limit Theorem (CLT), informally:}
      \begin{itemize}
        \item the error $\widehat{I}_N - \E[f(X)]$ behaves like a Gaussian
              with width $\sim 1/\sqrt{N}$,
        \item so typical error size is about a constant times $1/\sqrt{N}$.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Monte Carlo: error scaling}
  \begin{itemize}
    \item Assume $\Var(f(X)) = \sigma^2 < \infty$.
    \item Then
      \[
        \Var(\widehat{I}_N)
        = \Var\!\left(\frac{1}{N}\sum_{i=1}^N f(X_i)\right)
        = \frac{\sigma^2}{N}.
      \]
    \item Standard deviation (typical error size):
      \[
        \sqrt{\Var(\widehat{I}_N)} = \frac{\sigma}{\sqrt{N}}.
      \]
    \item \textbf{Key point:}
      \[
        \text{MC error} = O\bigl(N^{-1/2}\bigr),
      \]
      with an exponent independent of $d$ (though $\sigma$ may depend on $d$).
  \end{itemize}
\end{frame}

\begin{frame}{Quasi-Monte Carlo (QMC)}
  \begin{itemize}
    \item Replace random points $X_i$ by a \emph{low-discrepancy sequence}
      $(x_i)_{i\ge 1} \subset [0,1]^d$:
      \[
        \widehat{I}_N^{\QMC}(f,d) = \frac{1}{N}\sum_{i=1}^N f(x_i).
      \]
    \item Sobol sequence: base-2 digital net designed to fill $[0,1]^d$
          very uniformly.
    \item For functions of bounded variation (Hardy--Krause),
      Koksma--Hlawka inequality gives
      \[
        \bigl|\widehat{I}_N^{\QMC} - I(f,d)\bigr|
        \le V_{\mathrm{HK}}(f)\,D^*(x_1,\dots,x_N),
      \]
      where $D^*$ is the star-discrepancy.
    \item For Sobol and related sequences,
      \[
        D^*(x_1,\dots,x_N) = O\!\left(\frac{(\log N)^d}{N}\right).
      \]
  \end{itemize}
\end{frame}

\begin{frame}{What is a Sobol sequence?}
  \begin{itemize}
    \item A \textbf{Sobol sequence} is a \emph{low-discrepancy sequence}
          of points in $[0,1]^d$:
      \begin{itemize}
        \item deterministic (not i.i.d.\ random),
        \item designed to be \textbf{very evenly spread out} over the cube.
      \end{itemize}
    \item Built in base 2 as a \emph{digital sequence}:
      \begin{itemize}
        \item in 1D, think of a base-2 van der Corput sequence
              (points that successively fill in the gaps),
        \item in higher $d$, Sobol chooses coordinates so that all
              low-dimensional projections are well distributed.
      \end{itemize}
    \item Key property: for $N$ points,
      \[
        D^*(x_1,\dots,x_N) = O\!\bigl((\log N)^d / N\bigr),
      \]
      so the points cover $[0,1]^d$ much more uniformly than i.i.d.\ samples.
  \end{itemize}
\end{frame}


\begin{frame}{MC vs QMC: heuristic comparison}
  \begin{itemize}
    \item Ignoring $(\log N)^d$:
      \[
        \text{MC: } O(N^{-1/2}), \qquad
        \text{QMC: } O(N^{-1}).
      \]
    \item In practice, QMC often gives
      \[
        \text{much smaller error than MC for the same } N
      \]
      when
      \begin{itemize}
        \item $f$ is smooth,
        \item the \emph{effective dimension} is moderate.
      \end{itemize}
    \item But constants and effective dimension matter a lot in high $d$.
    \item This is exactly what we explore numerically.
  \end{itemize}
\end{frame}

\begin{frame}{Error metrics}
  For each method (MC or QMC) we consider:
  \begin{itemize}
    \item \textbf{Absolute error}
      \[
        E_N(f,d) = \bigl|\widehat{I}_N(f,d) - I(f,d)\bigr|.
      \]
    \item \textbf{Error vs sample size}:
      \begin{itemize}
        \item log--log plots of $E_N$ vs $N$,
        \item estimate slopes (empirical convergence rates).
      \end{itemize}
    \item \textbf{Error vs wall-clock time}:
      \begin{itemize}
        \item time-to-accuracy plots $E_N$ vs $T_N$,
        \item compare which method reaches a target tolerance faster.
      \end{itemize}
    \item For MC, we also look at variability over repeated runs.
  \end{itemize}
\end{frame}

% =====================================================
\section{Test Integrands and Setup}

\begin{frame}{Dimensions and sample sizes}
  \begin{itemize}
    \item Dimensions:
      \[
        d \in \{5, 10, 15, 20\}.
      \]
    \item Sample sizes:
      \[
        N \in \{2^7, 2^8, \dots, 2^{15}\}
        = \{128, 256, \dots, 32768\}.
      \]
    \item For each $(f,d,N)$:
      \begin{itemize}
        \item MC: $R = 30$ independent runs,
        \item QMC: single Sobol net of size $N$ (via
              \texttt{random\_base2} in SciPy).
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Separable test integrands}
  \begin{itemize}
    \item We choose $f : [0,1]^d \to \R$ with closed-form integrals.
    \item First three integrands are \emph{separable}:
      \[
        f(x) = \prod_{j=1}^d g_j(x_j).
      \]
      Then
      \[
        I(f,d) = \prod_{j=1}^d \int_0^1 g_j(x_j)\,dx_j.
      \]
    \item This makes exact values cheap to compute while still giving
          nontrivial high-dimensional behavior.
  \end{itemize}
\end{frame}

\begin{frame}{Integrand A: separable exponential}
  \begin{itemize}
    \item Define
      \[
        f_A(x) = \exp\!\Bigl(-\sum_{j=1}^d x_j\Bigr)
               = \prod_{j=1}^d e^{-x_j}.
      \]
    \item Exact integral:
      \[
        I(f_A,d)
        = \prod_{j=1}^d \bigl(1 - e^{-1}\bigr)
        = \bigl(1 - e^{-1}\bigr)^d.
      \]
    \item Smooth, bounded, symmetric in all coordinates.
  \end{itemize}
\end{frame}

\begin{frame}{Integrand B: rational with arctan closed form}
  \begin{itemize}
    \item Define
      \[
        f_B(x) = \prod_{j=1}^d \frac{1}{1 + j x_j^2}.
      \]
    \item 1D factor:
      \[
        \int_0^1 \frac{1}{1 + j x^2}\,dx
        = \frac{1}{\sqrt{j}} \arctan(\sqrt{j}).
      \]
    \item So
      \[
        I(f_B,d) = \prod_{j=1}^d
        \frac{1}{\sqrt{j}} \arctan(\sqrt{j}).
      \]
    \item Smooth, but anisotropic: different behavior in different
          coordinate directions.
  \end{itemize}
\end{frame}

\begin{frame}{Integrand C: Gaussian-type}
  \begin{itemize}
    \item Define
      \[
        f_C(x) = \exp\!\Bigl(-\sum_{j=1}^d x_j^2\Bigr)
               = \prod_{j=1}^d e^{-x_j^2}.
      \]
    \item Use the error function:
      \[
        \int_0^1 e^{-x^2}\,dx
        = \frac{\sqrt{\pi}}{2}\,\mathrm{erf}(1).
      \]
    \item Hence
      \[
        I(f_C,d)
        = \left(\frac{\sqrt{\pi}}{2}\,\mathrm{erf}(1)\right)^d.
      \]
    \item Smooth, strongly related to Gaussian integrals.
  \end{itemize}
\end{frame}

\begin{frame}{Integrand D: low effective dimension}
  \begin{itemize}
    \item Define, for $d \ge 5$,
      \[
        f_D(x) = \left(\frac{1}{5}\sum_{j=1}^5 x_j\right)^2.
      \]
    \item Depends only on first five coordinates $\Rightarrow$
          \emph{low effective dimension}.
    \item Let $X_1,\dots,X_5 \sim \mathrm{Unif}(0,1)$ independent,
          $S = X_1 + \cdots + X_5$.
    \item Then
      \[
        I(f_D,d) = \E\!\left[\left(\frac{S}{5}\right)^2\right]
        = \frac{1}{25}\bigl(\Var(S) + (\E S)^2\bigr)
        = \frac{4}{15},
      \]
      independent of $d$.
    \item Designed to be especially favorable to QMC.
  \end{itemize}
\end{frame}


\section{Pseudocode (High-Level Overview)}

\begin{frame}{Plain Monte Carlo: high-level algorithm}
  \begin{block}{Goal}
    Approximate
    \[
      I(f,d) = \int_{[0,1]^d} f(x)\,dx
    \]
    using random samples.
  \end{block}

  \begin{block}{Pseudocode}
    \begin{enumerate}
      \item Input: integrand $f$, dimension $d$, sample size $N$.
      \item Set \texttt{sum} $\gets 0$.
      \item For $i = 1, \dots, N$:
        \begin{enumerate}
          \item draw $X_i \sim \mathrm{Unif}([0,1]^d)$\\
                (e.g.\ each coordinate $X_i[j]$ is a uniform random number in $[0,1]$),
          \item \texttt{sum} $\gets$ \texttt{sum} $+ f(X_i)$.
        \end{enumerate}
      \item Output estimate
        \[
          \widehat{I}_N^{\MC} = \frac{\texttt{sum}}{N}.
        \]
    \end{enumerate}
  \end{block}

  \small
  Randomness enters only in the step “draw $X_i$ uniformly on $[0,1]^d$”.
  The LLN/CLT tell us that the fluctuations
  $\widehat{I}_N^{\MC} - \E[f(X)]$ shrink like $1/\sqrt{N}$.
\end{frame}


\begin{frame}{Sobol Quasi--Monte Carlo: high-level algorithm}
  \begin{block}{Idea}
    Use a deterministic low-discrepancy sequence instead of random points.
  \end{block}

  \begin{block}{Pseudocode}
    \begin{enumerate}
      \item Input: integrand $f$, dimension $d$, sample size $N$.
      \item Construct a Sobol sequence generator in dimension $d$.
      \item Set \texttt{sum} $\gets 0$.
      \item For $i = 1, \dots, N$:
        \begin{enumerate}
          \item take the next point $x_i$ from the Sobol sequence in $[0,1]^d$,
          \item \texttt{sum} $\gets$ \texttt{sum} $+ f(x_i)$.
        \end{enumerate}
      \item Output estimate
        \[
          \widehat{I}_N^{\QMC} = \frac{\texttt{sum}}{N}.
        \]
    \end{enumerate}
  \end{block}

  \small
  Here there is no randomness: once you fix “Sobol” and the dimension $d$,
  the sequence $(x_i)$ is deterministic. The hope is that these points
  cover $[0,1]^d$ so evenly that the error behaves like $\text{const}/N$.
\end{frame}



% =====================================================
\section{Numerical Results}

\begin{frame}{Error vs $N$: moderate dimension ($d=5$)}
  \begin{itemize}
    \item Example: $f_A$ in $d=5$.
  \end{itemize}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{\figpath error_vs_N_integrand_A_d5.png}
  \end{center}
  \begin{itemize}
    \item QMC curve lies below MC mean error for moderate and large $N$.
    \item On log--log scale:
      \begin{itemize}
        \item QMC: slope $\approx -1$,
        \item MC: slope $\approx -1/2$ with more variability.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Error vs $N$: high dimension ($d=20$)}
  \begin{columns}
    \column{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{\figpath error_vs_N_integrand_A_d20.png}
      \end{center}
      \begin{center}
        $f_A$ in $d=20$
      \end{center}
    \column{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{\figpath error_vs_N_integrand_B_d20.png}
      \end{center}
      \begin{center}
        $f_B$ in $d=20$
      \end{center}
  \end{columns}
  \vspace{0.2cm}
  \begin{itemize}
    \item QMC still shows $\approx N^{-1}$-like decay but with a \emph{large constant}.
    \item MC errors start much smaller and stay below QMC for all $N$ tested.
  \end{itemize}
\end{frame}

\begin{frame}{Error vs $N$: Gaussian-type $f_C$ in $d=20$}
  \begin{center}
    \includegraphics[width=0.7\textwidth]{\figpath error_vs_N_integrand_C_d20.png}
  \end{center}
  \begin{itemize}
    \item For small $N$: MC error $<$ QMC error (pre-asymptotic regime).
    \item As $N$ grows: QMC decays faster, eventually catching up and slightly
          overtaking MC.
    \item Clear example of asymptotic QMC advantage showing up only at larger $N$.
  \end{itemize}
\end{frame}

\begin{frame}{Error vs $N$: low effective dimension $f_D$}
  \begin{columns}
    \column{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{\figpath error_vs_N_integrand_D_d5.png}
      \end{center}
      \begin{center}
        $f_D$ in $d=5$
      \end{center}
    \column{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{\figpath error_vs_N_integrand_D_d20.png}
      \end{center}
      \begin{center}
        $f_D$ in $d=20$
      \end{center}
  \end{columns}
  \vspace{0.2cm}
  \begin{itemize}
    \item QMC clearly dominates MC in both $d=5$ and $d=20$.
    \item QMC error curves in $d=5$ and $d=20$ are very similar:
          effective dimension $\approx 5$.
    \item Textbook example of QMC’s $O(N^{-1})$-type behavior.
  \end{itemize}
\end{frame}

\begin{frame}{Error vs time: representative examples}
  \begin{itemize}
    \item For each configuration, we measure wall-clock time $T_N$.
    \item Plots of $E_N$ vs $T_N$ mirror the $E_N$ vs $N$ behavior:
      \begin{itemize}
        \item When QMC has smaller error at fixed $N$ (e.g.\ $f_A$ in $d=5$,
              $f_D$), it also reaches a target tolerance faster.
        \item When MC dominates in error (e.g.\ $f_A$ and $f_B$ in $d=20$),
              MC is also more efficient in time.
      \end{itemize}
    \item Cost per sample is similar for MC and QMC, so accuracy differences
          translate directly into time-to-accuracy.
  \end{itemize}
\end{frame}

\begin{frame}{Error vs time: illustrative examples}
  \begin{columns}
    \column{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{\figpath error_vs_time_integrand_A_d5.png}\\[0.2em]
        \small $f_A$ in $d=5$\\
        \small QMC reaches target accuracy faster
      \end{center}

    \column{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{\figpath error_vs_time_integrand_A_d20.png}\\[0.2em]
        \small $f_A$ in $d=20$\\
        \small MC more efficient over this range
      \end{center}
  \end{columns}

  \vspace{0.3cm}
  \small
  Same cost per sample for MC and QMC $\Rightarrow$ the lower curve at a given
  time is the more efficient method.
\end{frame}




% =====================================================
\section{Discussion and Takeaways}

\begin{frame}{Summary of findings}
  \begin{itemize}
    \item \textbf{Moderate dimension ($d=5$):}
      \begin{itemize}
        \item QMC outperforms MC on all three separable integrands $f_A,f_B,f_C$.
        \item Empirical slopes: QMC $\approx -1$, MC $\approx -1/2$.
      \end{itemize}
    \item \textbf{High dimension ($d=20$):}
      \begin{itemize}
        \item For $f_A$ and $f_B$ (very small exact integrals),
              MC has smaller error than QMC for all $N$.
        \item For Gaussian-type $f_C$, MC is better for small $N$,
              QMC catches up at larger $N$.
      \end{itemize}
    \item \textbf{Low effective dimension ($f_D$):}
      \begin{itemize}
        \item QMC clearly wins in both $d=5$ and $d=20$.
        \item Error curves essentially independent of the nominal dimension.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Rates, constants, and effective dimension}
  \begin{itemize}
    \item Asymptotic statements
      \[
        \text{MC } O(N^{-1/2}), \quad \text{QMC } O(N^{-1})
      \]
      hide important constants.
    \item For $f_A$ and $f_B$ in $d=20$:
      \begin{itemize}
        \item exact integrals are extremely small,
        \item MC errors inherit this small scale via the variance,
        \item QMC behaves more like $C/N$ with $C \approx O(1)$.
      \end{itemize}
    \item Effective dimension matters:
      \begin{itemize}
        \item when most variation is in a few coordinates (like $f_D$),
              QMC works extremely well,
        \item when variation is spread across many coordinates, the benefit
              can be muted or delayed.
      \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Practical guidelines}
  \begin{itemize}
    \item Use Sobol QMC when:
      \begin{itemize}
        \item the integrand is smooth,
        \item effective dimension is moderate or low,
        \item you can afford moderately large $N$.
      \end{itemize}
    \item Plain MC can be competitive or better when:
      \begin{itemize}
        \item dimension is very high,
        \item exact integrals / variances are extremely small,
        \item you are restricted to relatively small $N$.
      \end{itemize}
    \item Overall message:
      \[
        \text{“QMC is great, but problem structure really matters.”}
      \]
  \end{itemize}
\end{frame}

% =====================================================
\section{Implementation Notes}

\begin{frame}{Python implementation (Code/)}
  \begin{itemize}
    \item \texttt{integrands.py}
      \begin{itemize}
        \item definitions of $f_A, f_B, f_C, f_D$,
        \item closed-form formulas for $I(f,d)$.
      \end{itemize}
    \item \texttt{mc\_qmc.py}
      \begin{itemize}
        \item plain MC estimator using NumPy random sampling,
        \item Sobol QMC estimator using \texttt{scipy.stats.qmc.Sobol}.
      \end{itemize}
    \item \texttt{run\_experiments.py}
      \begin{itemize}
        \item loops over $f$, $d$, $N$,
        \item performs $R=30$ MC runs and 1 QMC run,
        \item writes results to \texttt{results\_mc\_qmc.csv}.
      \end{itemize}
    \item \texttt{plot\_results.py}
      \begin{itemize}
        \item reads \texttt{results\_mc\_qmc.csv},
        \item generates the error-vs-$N$ and error-vs-time plots.
      \end{itemize}
  \end{itemize}
\end{frame}


% =====================================================
\section*{References}

\begin{frame}{References}
  \footnotesize
  \begin{itemize}
    \item H.~Niederreiter,
      \emph{Random Number Generation and Quasi--Monte Carlo Methods},
      SIAM, 1992.

    \item C.~Lemieux,
      \emph{Monte Carlo and Quasi--Monte Carlo Sampling},
      Springer, 2009.

    \item A.~Quarteroni, R.~Sacco, and F.~Saleri,
      \emph{Numerical Mathematics}, 2nd ed.,
      Springer, 2007.

    \item A.~B.~Owen,
      \emph{Monte Carlo Theory, Methods and Examples},
      2013. Available online at
      \url{https://artowen.su.domains/mc/}.
  \end{itemize}
\end{frame}


% =====================================================
\begin{frame}
  \centering
  \vfill
  {\LARGE Questions?}
  \vfill
\end{frame}

\end{document}
